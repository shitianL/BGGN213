---
title: "Machine Learning 1"
author: 'Shitian Li (PID: A13294481)'
date: "10/22/2021"
output: pdf_document
---

# Clustering methods

Kmeansclustering in R is done with the `kmeans()` function.  
Here we make up some data to test and learn with. 

```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3))
data <- cbind(tmp, rev(tmp))
plot(data)
```

Run `kmeans()` set k (# of centers) to 2 and nstart (iteration) to 20. The thing with `kmeans()` is that you need to tell it how many clusters you want. 
```{r}
km <- kmeans(data, centers=2, nstart=20)
km
```

> Q. How many points are in each cluster?   

```{r}
km$size
```

> Q. What 'component' of your result object details cluster alignment/membership?

```{r}
km$cluster
```

> Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```

> Q. Plot x colored by the kemans cluster assignment and add cluster centers as blue points. 

```{r}
plot(data, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

# Hierarchical Clustering

We will use the `hclust()` function on the same data as before and see how this method works. 

```{r}
hc <- hclust(dist(data))
hc
```

```{r}
plot(hc)
```

To find our membership vector we need to "cut" the tree and for this we use the `cutree()` function and tell it the height to cut at. 

```{r}
plot(hc)
abline(h=7, col="red")
```

```{r}
cutree(hc, h=7)
```

We can also use `cutree()` and state the number of k clusters we want: 

```{r}
grps <- cutree(hc, k=2)
plot(data, col=grps)
```



# Principal Component Analysis (PCA)

Import the data from a CSV file

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

> Q. How many rows and cols? 

```{r}
dim(x)
```

> Q. How do we inspect and clean up the data? 

```{r}
rownames(x) <- x[,1]
x <- x[, -1]
x
```

Or we can read the data properly: 

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```

```{r}
barplot(as.matrix(x), col=rainbow(17))
```

```{r}
mycols <- rainbow(nrow(x))
pairs(x, col=mycols)
```


Anyways... None of these plots make sense. 


## PCA to the rescue

Here we will use the base R function for PCA, which is called `prcomp()`. This function wants the transpose of our data. 

```{r}
pca <- prcomp(t(x))
summary(pca)
```

```{r}
plot(pca)
```

We want score plot(aka PCA plot). Basically, of PC1 vs PC2

```{r}
attributes(pca)
```

```{r}
plot(pca$x[, 1:2])
text(pca$x[, 1:2], labels=colnames(x))
```

We can also examine the PCA "loadings", which tell us how much the original variables contribute to each PC. 

```{r}
barplot(pca$rotation[,1], las=2)
```


## PCA baby one more time!!! 

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

> Q. How many genes and samples? 

```{r}
nrow(rna.data)
```

```{r}
ncol(rna.data)
```

```{r}
colnames(rna.data)
```

```{r}
pca.rna <- prcomp(t(rna.data), scale=T)
summary(pca.rna)
```

Note here that nearly 93% of feature is captured by PC1. 

```{r}
plot(pca.rna)
```


```{r}
plot(pca.rna$x[, 1:2])
text(pca.rna$x[, 1:2], labels=colnames(rna.data))
```

```{r}
library(ggplot2)

df <- as.data.frame(pca.rna$x)

# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```

```{r}
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

pca.var <- pca.rna$sdev^2

pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE) +
labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clealy seperates wild-type from knock-out samples",
            x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="BIMM143 example data") +
     theme_bw()
p
```




